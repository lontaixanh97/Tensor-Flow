{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD+mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/quanhm/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train_loss: 14.008430480957031\n",
      "step: 1, train_loss: 3.232260227203369\n",
      "step: 2, train_loss: 4.01450252532959\n",
      "step: 3, train_loss: 2.9002275466918945\n",
      "step: 4, train_loss: 2.1070008277893066\n",
      "step: 5, train_loss: 1.9160442352294922\n",
      "step: 6, train_loss: 1.711913824081421\n",
      "step: 7, train_loss: 1.160210371017456\n",
      "step: 8, train_loss: 1.379097819328308\n",
      "step: 9, train_loss: 0.6659513711929321\n",
      "step: 10, train_loss: 1.1420364379882812\n",
      "step: 11, train_loss: 0.6633274555206299\n",
      "step: 12, train_loss: 0.8447467684745789\n",
      "step: 13, train_loss: 0.8466216325759888\n",
      "step: 14, train_loss: 0.5403729677200317\n",
      "step: 15, train_loss: 0.6734892129898071\n",
      "step: 16, train_loss: 0.7682428359985352\n",
      "step: 17, train_loss: 0.6870162487030029\n",
      "step: 18, train_loss: 0.4518558084964752\n",
      "step: 19, train_loss: 0.4145524203777313\n",
      "step: 20, train_loss: 0.2626575231552124\n",
      "step: 21, train_loss: 0.22827431559562683\n",
      "step: 22, train_loss: 0.3983951508998871\n",
      "step: 23, train_loss: 0.34788060188293457\n",
      "step: 24, train_loss: 0.3448857367038727\n",
      "step: 25, train_loss: 0.6745729446411133\n",
      "step: 26, train_loss: 0.29528945684432983\n",
      "step: 27, train_loss: 0.514325737953186\n",
      "step: 28, train_loss: 0.2861081659793854\n",
      "step: 29, train_loss: 0.24474075436592102\n",
      "step: 30, train_loss: 0.379506915807724\n",
      "step: 31, train_loss: 0.5665558576583862\n",
      "step: 32, train_loss: 0.3460458219051361\n",
      "step: 33, train_loss: 0.48195821046829224\n",
      "step: 34, train_loss: 0.16216622292995453\n",
      "step: 35, train_loss: 0.21736162900924683\n",
      "step: 36, train_loss: 0.16520901024341583\n",
      "step: 37, train_loss: 0.2262648344039917\n",
      "step: 38, train_loss: 0.37075427174568176\n",
      "step: 39, train_loss: 0.6254760026931763\n",
      "step: 40, train_loss: 0.17313098907470703\n",
      "step: 41, train_loss: 0.41839599609375\n",
      "step: 42, train_loss: 0.24708375334739685\n",
      "step: 43, train_loss: 0.17873089015483856\n",
      "step: 44, train_loss: 0.19120889902114868\n",
      "step: 45, train_loss: 0.4015968143939972\n",
      "step: 46, train_loss: 0.3835524320602417\n",
      "step: 47, train_loss: 0.22519424557685852\n",
      "step: 48, train_loss: 0.3104502260684967\n",
      "step: 49, train_loss: 0.4872916340827942\n",
      "step: 50, train_loss: 0.29720231890678406\n",
      "step: 51, train_loss: 0.27295297384262085\n",
      "step: 52, train_loss: 0.42090705037117004\n",
      "step: 53, train_loss: 0.19441068172454834\n",
      "step: 54, train_loss: 0.28823935985565186\n",
      "step: 55, train_loss: 0.35170572996139526\n",
      "step: 56, train_loss: 0.1792130321264267\n",
      "step: 57, train_loss: 0.4086339473724365\n",
      "step: 58, train_loss: 0.29192662239074707\n",
      "step: 59, train_loss: 0.37595653533935547\n",
      "step: 60, train_loss: 0.3069591522216797\n",
      "step: 61, train_loss: 0.2306995391845703\n",
      "step: 62, train_loss: 0.33031460642814636\n",
      "step: 63, train_loss: 0.20139293372631073\n",
      "step: 64, train_loss: 0.21278181672096252\n",
      "step: 65, train_loss: 0.3548070192337036\n",
      "step: 66, train_loss: 0.2978454530239105\n",
      "step: 67, train_loss: 0.29123008251190186\n",
      "step: 68, train_loss: 0.1825411319732666\n",
      "step: 69, train_loss: 0.2885708212852478\n",
      "step: 70, train_loss: 0.19261965155601501\n",
      "step: 71, train_loss: 0.16791483759880066\n",
      "step: 72, train_loss: 0.49228012561798096\n",
      "step: 73, train_loss: 0.38063162565231323\n",
      "step: 74, train_loss: 0.29226207733154297\n",
      "step: 75, train_loss: 0.41087207198143005\n",
      "step: 76, train_loss: 0.16939696669578552\n",
      "step: 77, train_loss: 0.10327529907226562\n",
      "step: 78, train_loss: 0.28821220993995667\n",
      "step: 79, train_loss: 0.2657228112220764\n",
      "step: 80, train_loss: 0.20821356773376465\n",
      "step: 81, train_loss: 0.3020440340042114\n",
      "step: 82, train_loss: 0.14703530073165894\n",
      "step: 83, train_loss: 0.12132059037685394\n",
      "step: 84, train_loss: 0.20522159337997437\n",
      "step: 85, train_loss: 0.46233904361724854\n",
      "step: 86, train_loss: 0.3649037480354309\n",
      "step: 87, train_loss: 0.2380945086479187\n",
      "step: 88, train_loss: 0.2828598618507385\n",
      "step: 89, train_loss: 0.24601268768310547\n",
      "step: 90, train_loss: 0.8569954633712769\n",
      "step: 91, train_loss: 0.5878046154975891\n",
      "step: 92, train_loss: 0.2409103512763977\n",
      "step: 93, train_loss: 0.17123174667358398\n",
      "step: 94, train_loss: 0.24577541649341583\n",
      "step: 95, train_loss: 0.30426257848739624\n",
      "step: 96, train_loss: 0.38091203570365906\n",
      "step: 97, train_loss: 0.2395191192626953\n",
      "step: 98, train_loss: 0.24528886377811432\n",
      "step: 99, train_loss: 0.39344000816345215\n",
      "step: 100, train_loss: 0.26779425144195557\n",
      "step: 101, train_loss: 0.20108860731124878\n",
      "step: 102, train_loss: 0.5109390020370483\n",
      "step: 103, train_loss: 0.29550036787986755\n",
      "step: 104, train_loss: 0.1995818018913269\n",
      "step: 105, train_loss: 0.2377980649471283\n",
      "step: 106, train_loss: 0.5901073813438416\n",
      "step: 107, train_loss: 0.2578939199447632\n",
      "step: 108, train_loss: 0.2330697476863861\n",
      "step: 109, train_loss: 0.25142329931259155\n",
      "step: 110, train_loss: 0.25496432185173035\n",
      "step: 111, train_loss: 0.2820419371128082\n",
      "step: 112, train_loss: 0.2693171501159668\n",
      "step: 113, train_loss: 0.2131822109222412\n",
      "step: 114, train_loss: 0.4824828803539276\n",
      "step: 115, train_loss: 0.1652410626411438\n",
      "step: 116, train_loss: 0.5306444764137268\n",
      "step: 117, train_loss: 0.23816320300102234\n",
      "step: 118, train_loss: 0.15230849385261536\n",
      "step: 119, train_loss: 0.386565238237381\n",
      "step: 120, train_loss: 0.3253237009048462\n",
      "step: 121, train_loss: 0.2760193943977356\n",
      "step: 122, train_loss: 0.30364149808883667\n",
      "step: 123, train_loss: 0.1578446924686432\n",
      "step: 124, train_loss: 0.23008424043655396\n",
      "step: 125, train_loss: 0.33469805121421814\n",
      "step: 126, train_loss: 0.2884034812450409\n",
      "step: 127, train_loss: 0.3478529453277588\n",
      "step: 128, train_loss: 0.47322916984558105\n",
      "step: 129, train_loss: 0.5551734566688538\n",
      "step: 130, train_loss: 0.29393938183784485\n",
      "step: 131, train_loss: 0.17785172164440155\n",
      "step: 132, train_loss: 0.3198034465312958\n",
      "step: 133, train_loss: 0.46730607748031616\n",
      "step: 134, train_loss: 0.16189205646514893\n",
      "step: 135, train_loss: 0.1265287846326828\n",
      "step: 136, train_loss: 0.3638960123062134\n",
      "step: 137, train_loss: 0.1983492076396942\n",
      "step: 138, train_loss: 0.2292085587978363\n",
      "step: 139, train_loss: 0.31417614221572876\n",
      "step: 140, train_loss: 0.3407060503959656\n",
      "step: 141, train_loss: 0.2243747115135193\n",
      "step: 142, train_loss: 0.5639451742172241\n",
      "step: 143, train_loss: 0.2504870295524597\n",
      "step: 144, train_loss: 0.41706520318984985\n",
      "step: 145, train_loss: 0.30970829725265503\n",
      "step: 146, train_loss: 0.28075575828552246\n",
      "step: 147, train_loss: 0.24698111414909363\n",
      "step: 148, train_loss: 0.2531941533088684\n",
      "step: 149, train_loss: 0.15993598103523254\n",
      "step: 150, train_loss: 0.19869345426559448\n",
      "step: 151, train_loss: 0.4740351140499115\n",
      "step: 152, train_loss: 0.2915431559085846\n",
      "step: 153, train_loss: 0.3702624440193176\n",
      "step: 154, train_loss: 0.4377497434616089\n",
      "step: 155, train_loss: 0.2287352979183197\n",
      "step: 156, train_loss: 0.29174599051475525\n",
      "step: 157, train_loss: 0.2227841019630432\n",
      "step: 158, train_loss: 0.28530949354171753\n",
      "step: 159, train_loss: 0.5007327198982239\n",
      "step: 160, train_loss: 0.30004411935806274\n",
      "step: 161, train_loss: 0.30946803092956543\n",
      "step: 162, train_loss: 0.40311384201049805\n",
      "step: 163, train_loss: 0.29446670413017273\n",
      "step: 164, train_loss: 0.27049511671066284\n",
      "step: 165, train_loss: 0.3298223912715912\n",
      "step: 166, train_loss: 0.17636650800704956\n",
      "step: 167, train_loss: 0.27680331468582153\n",
      "step: 168, train_loss: 0.16933409869670868\n",
      "step: 169, train_loss: 0.33184874057769775\n",
      "step: 170, train_loss: 0.38495945930480957\n",
      "step: 171, train_loss: 0.25229406356811523\n",
      "step: 172, train_loss: 0.4315355122089386\n",
      "step: 173, train_loss: 0.231977641582489\n",
      "step: 174, train_loss: 0.12581849098205566\n",
      "step: 175, train_loss: 0.2434023767709732\n",
      "step: 176, train_loss: 0.25835663080215454\n",
      "step: 177, train_loss: 0.3802962303161621\n",
      "step: 178, train_loss: 0.26109927892684937\n",
      "step: 179, train_loss: 0.2534750699996948\n",
      "step: 180, train_loss: 0.1806984543800354\n",
      "step: 181, train_loss: 0.5960503220558167\n",
      "step: 182, train_loss: 0.26433447003364563\n",
      "step: 183, train_loss: 0.5181881189346313\n",
      "step: 184, train_loss: 0.16059410572052002\n",
      "step: 185, train_loss: 0.2705370783805847\n",
      "step: 186, train_loss: 0.5323648452758789\n",
      "step: 187, train_loss: 0.2788624167442322\n",
      "step: 188, train_loss: 0.4400913417339325\n",
      "step: 189, train_loss: 0.3496430516242981\n",
      "step: 190, train_loss: 0.20784661173820496\n",
      "step: 191, train_loss: 0.29582324624061584\n",
      "step: 192, train_loss: 0.15790675580501556\n",
      "step: 193, train_loss: 0.24724066257476807\n",
      "step: 194, train_loss: 0.1327333152294159\n",
      "step: 195, train_loss: 0.1748860776424408\n",
      "step: 196, train_loss: 0.36042559146881104\n",
      "step: 197, train_loss: 0.4456719756126404\n",
      "step: 198, train_loss: 0.4761880934238434\n",
      "step: 199, train_loss: 0.4169374108314514\n",
      "step: 200, train_loss: 0.22074835002422333\n",
      "step: 201, train_loss: 0.14255332946777344\n",
      "step: 202, train_loss: 0.20486706495285034\n",
      "step: 203, train_loss: 0.18873974680900574\n",
      "step: 204, train_loss: 0.22116242349147797\n",
      "step: 205, train_loss: 0.23611749708652496\n",
      "step: 206, train_loss: 0.3147462010383606\n",
      "step: 207, train_loss: 0.22717857360839844\n",
      "step: 208, train_loss: 0.21667779982089996\n",
      "step: 209, train_loss: 0.16211453080177307\n",
      "step: 210, train_loss: 0.17689603567123413\n",
      "step: 211, train_loss: 0.17804789543151855\n",
      "step: 212, train_loss: 0.633086621761322\n",
      "step: 213, train_loss: 0.42972806096076965\n",
      "step: 214, train_loss: 0.1770395040512085\n",
      "step: 215, train_loss: 0.18243278563022614\n",
      "step: 216, train_loss: 0.2419491708278656\n",
      "step: 217, train_loss: 0.25447317957878113\n",
      "step: 218, train_loss: 0.24099889397621155\n",
      "step: 219, train_loss: 0.20527204871177673\n",
      "step: 220, train_loss: 0.427344411611557\n",
      "step: 221, train_loss: 0.29099440574645996\n",
      "step: 222, train_loss: 0.27033352851867676\n",
      "step: 223, train_loss: 0.22227780520915985\n",
      "step: 224, train_loss: 0.36906754970550537\n",
      "step: 225, train_loss: 0.22466403245925903\n",
      "step: 226, train_loss: 0.23616132140159607\n",
      "step: 227, train_loss: 0.5637803077697754\n",
      "step: 228, train_loss: 0.16015373170375824\n",
      "step: 229, train_loss: 0.2444573938846588\n",
      "step: 230, train_loss: 0.3729625642299652\n",
      "step: 231, train_loss: 0.3985968828201294\n",
      "step: 232, train_loss: 0.3457924723625183\n",
      "step: 233, train_loss: 0.3747606873512268\n",
      "step: 234, train_loss: 0.25613635778427124\n",
      "step: 235, train_loss: 0.20684418082237244\n",
      "step: 236, train_loss: 0.1888696253299713\n",
      "step: 237, train_loss: 0.1686866283416748\n",
      "step: 238, train_loss: 0.5135422348976135\n",
      "step: 239, train_loss: 0.2972279489040375\n",
      "step: 240, train_loss: 0.5058950781822205\n",
      "step: 241, train_loss: 0.22282254695892334\n",
      "step: 242, train_loss: 0.4011164605617523\n",
      "step: 243, train_loss: 0.26800650358200073\n",
      "step: 244, train_loss: 0.09696377068758011\n",
      "step: 245, train_loss: 0.24663004279136658\n",
      "step: 246, train_loss: 0.17160451412200928\n",
      "step: 247, train_loss: 0.15970179438591003\n",
      "step: 248, train_loss: 0.23336495459079742\n",
      "step: 249, train_loss: 0.24705953896045685\n",
      "step: 250, train_loss: 0.393960177898407\n",
      "step: 251, train_loss: 0.24285221099853516\n",
      "step: 252, train_loss: 0.4572366178035736\n",
      "step: 253, train_loss: 0.2600182294845581\n",
      "step: 254, train_loss: 0.16062191128730774\n",
      "step: 255, train_loss: 0.30957847833633423\n",
      "step: 256, train_loss: 0.3244625926017761\n",
      "step: 257, train_loss: 0.3795255124568939\n",
      "step: 258, train_loss: 0.27012646198272705\n",
      "step: 259, train_loss: 0.33610790967941284\n",
      "step: 260, train_loss: 0.3894667327404022\n",
      "step: 261, train_loss: 0.24357843399047852\n",
      "step: 262, train_loss: 0.14160802960395813\n",
      "step: 263, train_loss: 0.33289945125579834\n",
      "step: 264, train_loss: 0.2744619846343994\n",
      "step: 265, train_loss: 0.1834203451871872\n",
      "step: 266, train_loss: 0.5022457838058472\n",
      "step: 267, train_loss: 0.18665343523025513\n",
      "step: 268, train_loss: 0.13791894912719727\n",
      "step: 269, train_loss: 0.5508440732955933\n",
      "step: 270, train_loss: 0.2954723834991455\n",
      "step: 271, train_loss: 0.15273559093475342\n",
      "step: 272, train_loss: 0.3724457621574402\n",
      "step: 273, train_loss: 0.11875537037849426\n",
      "step: 274, train_loss: 0.3409838378429413\n",
      "step: 275, train_loss: 0.4426540434360504\n",
      "step: 276, train_loss: 0.32676857709884644\n",
      "step: 277, train_loss: 0.3825361132621765\n",
      "step: 278, train_loss: 0.22904740273952484\n",
      "step: 279, train_loss: 0.20116814970970154\n",
      "step: 280, train_loss: 0.307016521692276\n",
      "step: 281, train_loss: 0.39109885692596436\n",
      "step: 282, train_loss: 0.4445842504501343\n",
      "step: 283, train_loss: 0.41721561551094055\n",
      "step: 284, train_loss: 0.26106715202331543\n",
      "step: 285, train_loss: 0.3179500699043274\n",
      "step: 286, train_loss: 0.21191298961639404\n",
      "step: 287, train_loss: 0.716458797454834\n",
      "step: 288, train_loss: 0.33374083042144775\n",
      "step: 289, train_loss: 0.41207993030548096\n",
      "step: 290, train_loss: 0.3545418679714203\n",
      "step: 291, train_loss: 0.1659557819366455\n",
      "step: 292, train_loss: 0.25134697556495667\n",
      "step: 293, train_loss: 0.19605417549610138\n",
      "step: 294, train_loss: 0.2814507484436035\n",
      "step: 295, train_loss: 0.3069097697734833\n",
      "step: 296, train_loss: 0.3291935920715332\n",
      "step: 297, train_loss: 0.23842111229896545\n",
      "step: 298, train_loss: 0.1489013433456421\n",
      "step: 299, train_loss: 0.18842105567455292\n",
      "step: 300, train_loss: 0.3743743300437927\n",
      "step: 301, train_loss: 0.5465300679206848\n",
      "step: 302, train_loss: 0.2974083423614502\n",
      "step: 303, train_loss: 0.1391477882862091\n",
      "step: 304, train_loss: 0.2962627708911896\n",
      "step: 305, train_loss: 0.25537174940109253\n",
      "step: 306, train_loss: 0.30702558159828186\n",
      "step: 307, train_loss: 0.12479054927825928\n",
      "step: 308, train_loss: 0.21243852376937866\n",
      "step: 309, train_loss: 0.34139496088027954\n",
      "step: 310, train_loss: 0.14539358019828796\n",
      "step: 311, train_loss: 0.8048802614212036\n",
      "step: 312, train_loss: 0.39042460918426514\n",
      "step: 313, train_loss: 0.12489236891269684\n",
      "step: 314, train_loss: 0.1870657354593277\n",
      "step: 315, train_loss: 0.21693074703216553\n",
      "step: 316, train_loss: 0.3855646848678589\n",
      "step: 317, train_loss: 0.5852181315422058\n",
      "step: 318, train_loss: 0.3459886610507965\n",
      "step: 319, train_loss: 0.38252782821655273\n",
      "step: 320, train_loss: 0.46737703680992126\n",
      "step: 321, train_loss: 0.22648438811302185\n",
      "step: 322, train_loss: 0.16338381171226501\n",
      "step: 323, train_loss: 0.1463426649570465\n",
      "step: 324, train_loss: 0.23863133788108826\n",
      "step: 325, train_loss: 0.23338842391967773\n",
      "step: 326, train_loss: 0.15558138489723206\n",
      "step: 327, train_loss: 0.2157619595527649\n",
      "step: 328, train_loss: 0.2632749080657959\n",
      "step: 329, train_loss: 0.15199464559555054\n",
      "step: 330, train_loss: 0.23581458628177643\n",
      "step: 331, train_loss: 0.4761185944080353\n",
      "step: 332, train_loss: 0.23746871948242188\n",
      "step: 333, train_loss: 0.41677626967430115\n",
      "step: 334, train_loss: 0.24543526768684387\n",
      "step: 335, train_loss: 0.4241334795951843\n",
      "step: 336, train_loss: 0.37857717275619507\n",
      "step: 337, train_loss: 0.1616308093070984\n",
      "step: 338, train_loss: 0.27044904232025146\n",
      "step: 339, train_loss: 0.3077325224876404\n",
      "step: 340, train_loss: 0.10600288212299347\n",
      "step: 341, train_loss: 0.26603764295578003\n",
      "step: 342, train_loss: 0.15960180759429932\n",
      "step: 343, train_loss: 0.1835443079471588\n",
      "step: 344, train_loss: 0.563439130783081\n",
      "step: 345, train_loss: 0.3745034337043762\n",
      "step: 346, train_loss: 0.2221461534500122\n",
      "step: 347, train_loss: 0.3463824391365051\n",
      "step: 348, train_loss: 0.18586760759353638\n",
      "step: 349, train_loss: 0.3064338266849518\n",
      "step: 350, train_loss: 0.3577234148979187\n",
      "step: 351, train_loss: 0.22837859392166138\n",
      "step: 352, train_loss: 0.25764647126197815\n",
      "step: 353, train_loss: 0.4390121102333069\n",
      "step: 354, train_loss: 0.2154117375612259\n",
      "step: 355, train_loss: 0.2710099220275879\n",
      "step: 356, train_loss: 0.26159965991973877\n",
      "step: 357, train_loss: 0.38599395751953125\n",
      "step: 358, train_loss: 0.2163226455450058\n",
      "step: 359, train_loss: 0.5190603733062744\n",
      "step: 360, train_loss: 0.2661530673503876\n",
      "step: 361, train_loss: 0.2725186347961426\n",
      "step: 362, train_loss: 0.2108335793018341\n",
      "step: 363, train_loss: 0.17734353244304657\n",
      "step: 364, train_loss: 0.16263453662395477\n",
      "step: 365, train_loss: 0.1740952730178833\n",
      "step: 366, train_loss: 0.17199069261550903\n",
      "step: 367, train_loss: 0.24547171592712402\n",
      "step: 368, train_loss: 0.47105932235717773\n",
      "step: 369, train_loss: 0.3882943391799927\n",
      "step: 370, train_loss: 0.24491769075393677\n",
      "step: 371, train_loss: 0.3847479522228241\n",
      "step: 372, train_loss: 0.2844492197036743\n",
      "step: 373, train_loss: 0.3462721109390259\n",
      "step: 374, train_loss: 0.20182137191295624\n",
      "step: 375, train_loss: 0.246221661567688\n",
      "step: 376, train_loss: 0.16100162267684937\n",
      "step: 377, train_loss: 0.2417624443769455\n",
      "step: 378, train_loss: 0.7506380677223206\n",
      "step: 379, train_loss: 0.1470395028591156\n",
      "step: 380, train_loss: 0.18408435583114624\n",
      "step: 381, train_loss: 0.3621187210083008\n",
      "step: 382, train_loss: 0.25996989011764526\n",
      "step: 383, train_loss: 0.14905261993408203\n",
      "step: 384, train_loss: 0.22475989162921906\n",
      "step: 385, train_loss: 0.16892924904823303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 386, train_loss: 0.17334899306297302\n",
      "step: 387, train_loss: 0.6522247195243835\n",
      "step: 388, train_loss: 0.13865801692008972\n",
      "step: 389, train_loss: 0.4219469130039215\n",
      "step: 390, train_loss: 0.18905581533908844\n",
      "step: 391, train_loss: 0.3926783502101898\n",
      "step: 392, train_loss: 0.24527069926261902\n",
      "step: 393, train_loss: 0.2804713249206543\n",
      "step: 394, train_loss: 0.34605276584625244\n",
      "step: 395, train_loss: 0.15872813761234283\n",
      "step: 396, train_loss: 0.1271091252565384\n",
      "step: 397, train_loss: 0.14514890313148499\n",
      "step: 398, train_loss: 0.46624553203582764\n",
      "step: 399, train_loss: 0.31294482946395874\n",
      "step: 400, train_loss: 0.3285251557826996\n",
      "step: 401, train_loss: 0.5208274126052856\n",
      "step: 402, train_loss: 0.21300990879535675\n",
      "step: 403, train_loss: 0.1602523922920227\n",
      "step: 404, train_loss: 0.1344030350446701\n",
      "step: 405, train_loss: 0.34937334060668945\n",
      "step: 406, train_loss: 0.10045763850212097\n",
      "step: 407, train_loss: 0.2658616304397583\n",
      "step: 408, train_loss: 0.2750292718410492\n",
      "step: 409, train_loss: 0.6741048097610474\n",
      "step: 410, train_loss: 0.44236838817596436\n",
      "step: 411, train_loss: 0.4799325168132782\n",
      "step: 412, train_loss: 0.23839524388313293\n",
      "step: 413, train_loss: 0.29587721824645996\n",
      "step: 414, train_loss: 0.22275634109973907\n",
      "step: 415, train_loss: 0.18457379937171936\n",
      "step: 416, train_loss: 0.24014852941036224\n",
      "step: 417, train_loss: 0.433958500623703\n",
      "step: 418, train_loss: 0.34964853525161743\n",
      "step: 419, train_loss: 0.29427409172058105\n",
      "step: 420, train_loss: 0.1765112578868866\n",
      "step: 421, train_loss: 0.207637220621109\n",
      "step: 422, train_loss: 0.46679773926734924\n",
      "step: 423, train_loss: 0.22872230410575867\n",
      "step: 424, train_loss: 0.23461541533470154\n",
      "step: 425, train_loss: 0.469590961933136\n",
      "step: 426, train_loss: 0.18697577714920044\n",
      "step: 427, train_loss: 0.26454633474349976\n",
      "step: 428, train_loss: 0.4253166615962982\n",
      "step: 429, train_loss: 0.19796591997146606\n",
      "step: 430, train_loss: 0.3431825637817383\n",
      "step: 431, train_loss: 0.27599257230758667\n",
      "step: 432, train_loss: 0.2451503723859787\n",
      "step: 433, train_loss: 0.2434210181236267\n",
      "step: 434, train_loss: 0.21912351250648499\n",
      "step: 435, train_loss: 0.4365087151527405\n",
      "step: 436, train_loss: 0.332598477602005\n",
      "step: 437, train_loss: 0.5676854848861694\n",
      "step: 438, train_loss: 0.20259034633636475\n",
      "step: 439, train_loss: 0.16097599267959595\n",
      "step: 440, train_loss: 0.4886409044265747\n",
      "step: 441, train_loss: 0.3345726430416107\n",
      "step: 442, train_loss: 0.3774031102657318\n",
      "step: 443, train_loss: 0.23091037571430206\n",
      "step: 444, train_loss: 0.14614646136760712\n",
      "step: 445, train_loss: 0.33951056003570557\n",
      "step: 446, train_loss: 0.17270414531230927\n",
      "step: 447, train_loss: 0.24257998168468475\n",
      "step: 448, train_loss: 0.28148153424263\n",
      "step: 449, train_loss: 0.22357788681983948\n",
      "step: 450, train_loss: 0.14331978559494019\n",
      "step: 451, train_loss: 0.22511717677116394\n",
      "step: 452, train_loss: 0.32838794589042664\n",
      "step: 453, train_loss: 0.3220862150192261\n",
      "step: 454, train_loss: 0.9423311948776245\n",
      "step: 455, train_loss: 0.4091925323009491\n",
      "step: 456, train_loss: 0.16661792993545532\n",
      "step: 457, train_loss: 0.2517993152141571\n",
      "step: 458, train_loss: 0.18031089007854462\n",
      "step: 459, train_loss: 0.22073079645633698\n",
      "step: 460, train_loss: 0.2511385679244995\n",
      "step: 461, train_loss: 0.24843956530094147\n",
      "step: 462, train_loss: 0.1937021017074585\n",
      "step: 463, train_loss: 0.23012319207191467\n",
      "step: 464, train_loss: 0.47257447242736816\n",
      "step: 465, train_loss: 0.245996356010437\n",
      "step: 466, train_loss: 0.5011656880378723\n",
      "step: 467, train_loss: 0.3892384469509125\n",
      "step: 468, train_loss: 0.44959700107574463\n",
      "step: 469, train_loss: 0.24893537163734436\n",
      "step: 470, train_loss: 0.32522523403167725\n",
      "step: 471, train_loss: 0.2935166358947754\n",
      "step: 472, train_loss: 0.16176563501358032\n",
      "step: 473, train_loss: 0.38608819246292114\n",
      "step: 474, train_loss: 0.14808289706707\n",
      "step: 475, train_loss: 0.12682154774665833\n",
      "step: 476, train_loss: 0.419575035572052\n",
      "step: 477, train_loss: 0.5571524500846863\n",
      "step: 478, train_loss: 0.14551842212677002\n",
      "step: 479, train_loss: 0.2670043408870697\n",
      "step: 480, train_loss: 0.2767064571380615\n",
      "step: 481, train_loss: 0.5768880844116211\n",
      "step: 482, train_loss: 0.19953972101211548\n",
      "step: 483, train_loss: 0.34791654348373413\n",
      "step: 484, train_loss: 0.2236647605895996\n",
      "step: 485, train_loss: 0.2422521412372589\n",
      "step: 486, train_loss: 0.545995831489563\n",
      "step: 487, train_loss: 0.3043637275695801\n",
      "step: 488, train_loss: 0.25676366686820984\n",
      "step: 489, train_loss: 0.16736853122711182\n",
      "step: 490, train_loss: 0.2667897343635559\n",
      "step: 491, train_loss: 0.3317018151283264\n",
      "step: 492, train_loss: 0.11603417992591858\n",
      "step: 493, train_loss: 0.1916532814502716\n",
      "step: 494, train_loss: 0.17789871990680695\n",
      "step: 495, train_loss: 0.12268868088722229\n",
      "step: 496, train_loss: 0.4284414052963257\n",
      "step: 497, train_loss: 0.3248187005519867\n",
      "step: 498, train_loss: 0.8225488662719727\n",
      "step: 499, train_loss: 0.14357773959636688\n",
      "step: 500, train_loss: 0.21061566472053528\n",
      "step: 501, train_loss: 0.2183433324098587\n",
      "step: 502, train_loss: 0.1822262406349182\n",
      "step: 503, train_loss: 0.25728845596313477\n",
      "step: 504, train_loss: 0.12889480590820312\n",
      "step: 505, train_loss: 0.30166539549827576\n",
      "step: 506, train_loss: 0.5412584543228149\n",
      "step: 507, train_loss: 0.22435316443443298\n",
      "step: 508, train_loss: 0.14783242344856262\n",
      "step: 509, train_loss: 0.30034172534942627\n",
      "step: 510, train_loss: 0.280534029006958\n",
      "step: 511, train_loss: 0.3348967730998993\n",
      "step: 512, train_loss: 0.27791059017181396\n",
      "step: 513, train_loss: 0.36480534076690674\n",
      "step: 514, train_loss: 0.3902857005596161\n",
      "step: 515, train_loss: 0.7759357690811157\n",
      "step: 516, train_loss: 0.28753161430358887\n",
      "step: 517, train_loss: 0.14611338078975677\n",
      "step: 518, train_loss: 0.22377635538578033\n",
      "step: 519, train_loss: 0.1450095921754837\n",
      "step: 520, train_loss: 0.1838466227054596\n",
      "step: 521, train_loss: 0.37491971254348755\n",
      "step: 522, train_loss: 0.28972506523132324\n",
      "step: 523, train_loss: 0.2753124237060547\n",
      "step: 524, train_loss: 0.5759421586990356\n",
      "step: 525, train_loss: 0.30116283893585205\n",
      "step: 526, train_loss: 0.3667699992656708\n",
      "step: 527, train_loss: 0.362571656703949\n",
      "step: 528, train_loss: 0.21720965206623077\n",
      "step: 529, train_loss: 0.2144065946340561\n",
      "step: 530, train_loss: 0.2275460660457611\n",
      "step: 531, train_loss: 0.23888131976127625\n",
      "step: 532, train_loss: 0.22985491156578064\n",
      "step: 533, train_loss: 0.2515377998352051\n",
      "step: 534, train_loss: 0.2190217822790146\n",
      "step: 535, train_loss: 0.15517930686473846\n",
      "step: 536, train_loss: 0.5354571342468262\n",
      "step: 537, train_loss: 0.4026568531990051\n",
      "step: 538, train_loss: 0.4257603883743286\n",
      "step: 539, train_loss: 0.25399041175842285\n",
      "step: 540, train_loss: 0.34183818101882935\n",
      "step: 541, train_loss: 0.2613627314567566\n",
      "step: 542, train_loss: 0.276552677154541\n",
      "step: 543, train_loss: 0.18252813816070557\n",
      "step: 544, train_loss: 0.36583203077316284\n",
      "step: 545, train_loss: 0.24775934219360352\n",
      "step: 546, train_loss: 0.4028470516204834\n",
      "step: 547, train_loss: 0.22431372106075287\n",
      "step: 548, train_loss: 0.09172046184539795\n",
      "step: 549, train_loss: 0.27887430787086487\n",
      "step: 550, train_loss: 0.5856161117553711\n",
      "step: 551, train_loss: 0.33830875158309937\n",
      "step: 552, train_loss: 0.2274053394794464\n",
      "step: 553, train_loss: 0.19589446485042572\n",
      "step: 554, train_loss: 0.34634584188461304\n",
      "step: 555, train_loss: 0.2595183849334717\n",
      "step: 556, train_loss: 0.3173847198486328\n",
      "step: 557, train_loss: 0.18473893404006958\n",
      "step: 558, train_loss: 0.15366128087043762\n",
      "step: 559, train_loss: 0.2414325326681137\n",
      "step: 560, train_loss: 0.4219541847705841\n",
      "step: 561, train_loss: 0.3136134743690491\n",
      "step: 562, train_loss: 0.21054577827453613\n",
      "step: 563, train_loss: 0.13390418887138367\n",
      "step: 564, train_loss: 0.3301246166229248\n",
      "step: 565, train_loss: 0.16929858922958374\n",
      "step: 566, train_loss: 0.3579866886138916\n",
      "step: 567, train_loss: 0.28114020824432373\n",
      "step: 568, train_loss: 0.2727682590484619\n",
      "step: 569, train_loss: 0.4377634525299072\n",
      "step: 570, train_loss: 0.2632547616958618\n",
      "step: 571, train_loss: 0.3290696144104004\n",
      "step: 572, train_loss: 0.3856331706047058\n",
      "step: 573, train_loss: 0.2266579419374466\n",
      "step: 574, train_loss: 0.2381618469953537\n",
      "step: 575, train_loss: 0.26540929079055786\n",
      "step: 576, train_loss: 0.44551587104797363\n",
      "step: 577, train_loss: 0.1885618269443512\n",
      "step: 578, train_loss: 0.3256784677505493\n",
      "step: 579, train_loss: 0.303444504737854\n",
      "step: 580, train_loss: 0.18145371973514557\n",
      "step: 581, train_loss: 0.3053164482116699\n",
      "step: 582, train_loss: 0.2533356249332428\n",
      "step: 583, train_loss: 0.33146002888679504\n",
      "step: 584, train_loss: 0.22785606980323792\n",
      "step: 585, train_loss: 0.20036405324935913\n",
      "step: 586, train_loss: 0.22564923763275146\n",
      "step: 587, train_loss: 0.1951906830072403\n",
      "step: 588, train_loss: 0.3135531544685364\n",
      "step: 589, train_loss: 0.16571149230003357\n",
      "step: 590, train_loss: 0.518076479434967\n",
      "step: 591, train_loss: 0.3738357126712799\n",
      "step: 592, train_loss: 0.44531476497650146\n",
      "step: 593, train_loss: 0.4879835844039917\n",
      "step: 594, train_loss: 0.21332748234272003\n",
      "step: 595, train_loss: 0.3700658679008484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 596, train_loss: 0.2911413908004761\n",
      "step: 597, train_loss: 0.1331476867198944\n",
      "step: 598, train_loss: 0.34271323680877686\n",
      "step: 599, train_loss: 0.31566277146339417\n",
      "step: 600, train_loss: 0.2940559983253479\n",
      "step: 601, train_loss: 0.20008929073810577\n",
      "step: 602, train_loss: 0.27096623182296753\n",
      "step: 603, train_loss: 0.2775583267211914\n",
      "step: 604, train_loss: 0.5009604096412659\n",
      "step: 605, train_loss: 0.2723824083805084\n",
      "step: 606, train_loss: 0.23215752840042114\n",
      "step: 607, train_loss: 0.25479376316070557\n",
      "step: 608, train_loss: 0.4163118898868561\n",
      "step: 609, train_loss: 0.20994526147842407\n",
      "step: 610, train_loss: 0.27825847268104553\n",
      "step: 611, train_loss: 0.24447865784168243\n",
      "step: 612, train_loss: 0.21700716018676758\n",
      "step: 613, train_loss: 0.23154279589653015\n",
      "step: 614, train_loss: 0.38820087909698486\n",
      "step: 615, train_loss: 0.2664022445678711\n",
      "step: 616, train_loss: 0.389880508184433\n",
      "step: 617, train_loss: 0.4533211290836334\n",
      "step: 618, train_loss: 0.43576347827911377\n",
      "step: 619, train_loss: 0.27537819743156433\n",
      "step: 620, train_loss: 0.26248157024383545\n",
      "step: 621, train_loss: 0.159318208694458\n",
      "step: 622, train_loss: 0.09384877979755402\n",
      "step: 623, train_loss: 0.30892953276634216\n",
      "step: 624, train_loss: 0.26967325806617737\n",
      "step: 625, train_loss: 0.1294853389263153\n",
      "step: 626, train_loss: 0.1138661801815033\n",
      "step: 627, train_loss: 0.1678628921508789\n",
      "step: 628, train_loss: 0.1835031807422638\n",
      "step: 629, train_loss: 0.31275367736816406\n",
      "step: 630, train_loss: 0.254778116941452\n",
      "step: 631, train_loss: 0.5334354639053345\n",
      "step: 632, train_loss: 0.47794675827026367\n",
      "step: 633, train_loss: 0.33776193857192993\n",
      "step: 634, train_loss: 0.22755685448646545\n",
      "step: 635, train_loss: 0.731140673160553\n",
      "step: 636, train_loss: 0.25191792845726013\n",
      "step: 637, train_loss: 0.329487144947052\n",
      "step: 638, train_loss: 0.5051379799842834\n",
      "step: 639, train_loss: 0.3875941336154938\n",
      "step: 640, train_loss: 0.1691717803478241\n",
      "step: 641, train_loss: 0.15362653136253357\n",
      "step: 642, train_loss: 0.12573681771755219\n",
      "step: 643, train_loss: 0.18594379723072052\n",
      "step: 644, train_loss: 0.19731049239635468\n",
      "step: 645, train_loss: 0.18626227974891663\n",
      "step: 646, train_loss: 0.5567724108695984\n",
      "step: 647, train_loss: 0.4071539044380188\n",
      "step: 648, train_loss: 0.4894278049468994\n",
      "step: 649, train_loss: 0.4553850293159485\n",
      "step: 650, train_loss: 0.47162654995918274\n",
      "step: 651, train_loss: 0.2661631107330322\n",
      "step: 652, train_loss: 0.155231773853302\n",
      "step: 653, train_loss: 0.453827440738678\n",
      "step: 654, train_loss: 0.3081880211830139\n",
      "step: 655, train_loss: 0.22470656037330627\n",
      "step: 656, train_loss: 0.1378471404314041\n",
      "step: 657, train_loss: 0.18341730535030365\n",
      "step: 658, train_loss: 0.5994609594345093\n",
      "step: 659, train_loss: 0.17870333790779114\n",
      "step: 660, train_loss: 0.2788606584072113\n",
      "step: 661, train_loss: 0.2355988472700119\n",
      "step: 662, train_loss: 0.35582414269447327\n",
      "step: 663, train_loss: 0.8917453289031982\n",
      "step: 664, train_loss: 0.26603972911834717\n",
      "step: 665, train_loss: 0.1944350004196167\n",
      "step: 666, train_loss: 0.28917059302330017\n",
      "step: 667, train_loss: 0.19109834730625153\n",
      "step: 668, train_loss: 0.1404656171798706\n",
      "step: 669, train_loss: 0.22820809483528137\n",
      "step: 670, train_loss: 0.7725808620452881\n",
      "step: 671, train_loss: 0.28127986192703247\n",
      "step: 672, train_loss: 0.2830529510974884\n",
      "step: 673, train_loss: 0.23321539163589478\n",
      "step: 674, train_loss: 0.21138986945152283\n",
      "step: 675, train_loss: 0.20723232626914978\n",
      "step: 676, train_loss: 0.25910621881484985\n",
      "step: 677, train_loss: 0.3170533776283264\n",
      "step: 678, train_loss: 0.13681036233901978\n",
      "step: 679, train_loss: 0.6393883228302002\n",
      "step: 680, train_loss: 0.47822946310043335\n",
      "step: 681, train_loss: 0.17819374799728394\n",
      "step: 682, train_loss: 0.15554946660995483\n",
      "step: 683, train_loss: 0.31564009189605713\n",
      "step: 684, train_loss: 0.5727272033691406\n",
      "step: 685, train_loss: 0.17840401828289032\n",
      "step: 686, train_loss: 0.21524153649806976\n",
      "step: 687, train_loss: 0.5147109031677246\n",
      "step: 688, train_loss: 0.20897522568702698\n",
      "step: 689, train_loss: 0.22224964201450348\n",
      "step: 690, train_loss: 0.35074788331985474\n",
      "step: 691, train_loss: 0.21290135383605957\n",
      "step: 692, train_loss: 0.3253665566444397\n",
      "step: 693, train_loss: 0.24722522497177124\n",
      "step: 694, train_loss: 0.6986123323440552\n",
      "step: 695, train_loss: 0.401749849319458\n",
      "step: 696, train_loss: 0.1993512213230133\n",
      "step: 697, train_loss: 0.10413146018981934\n",
      "step: 698, train_loss: 0.2513882517814636\n",
      "step: 699, train_loss: 0.21825075149536133\n",
      "step: 700, train_loss: 0.25891363620758057\n",
      "step: 701, train_loss: 0.513381838798523\n",
      "step: 702, train_loss: 0.25506091117858887\n",
      "step: 703, train_loss: 0.3271605372428894\n",
      "step: 704, train_loss: 0.4036235213279724\n",
      "step: 705, train_loss: 0.24067242443561554\n",
      "step: 706, train_loss: 0.27702420949935913\n",
      "step: 707, train_loss: 0.18049079179763794\n",
      "step: 708, train_loss: 0.2336636185646057\n",
      "step: 709, train_loss: 0.3159666061401367\n",
      "step: 710, train_loss: 0.23060809075832367\n",
      "step: 711, train_loss: 0.17217323184013367\n",
      "step: 712, train_loss: 0.30186524987220764\n",
      "step: 713, train_loss: 0.3469778895378113\n",
      "step: 714, train_loss: 0.31238865852355957\n",
      "step: 715, train_loss: 0.18800120055675507\n",
      "step: 716, train_loss: 0.1092853844165802\n",
      "step: 717, train_loss: 0.15505772829055786\n",
      "step: 718, train_loss: 0.21605074405670166\n",
      "step: 719, train_loss: 0.39434537291526794\n",
      "step: 720, train_loss: 0.2135838270187378\n",
      "step: 721, train_loss: 0.5228996276855469\n",
      "step: 722, train_loss: 0.15037815272808075\n",
      "step: 723, train_loss: 0.2425389289855957\n",
      "step: 724, train_loss: 0.4684840440750122\n",
      "step: 725, train_loss: 0.4299689829349518\n",
      "step: 726, train_loss: 0.6321295499801636\n",
      "step: 727, train_loss: 0.31412774324417114\n",
      "step: 728, train_loss: 0.17959293723106384\n",
      "step: 729, train_loss: 0.24481061100959778\n",
      "step: 730, train_loss: 0.42383643984794617\n",
      "step: 731, train_loss: 0.22388386726379395\n",
      "step: 732, train_loss: 0.5270906090736389\n",
      "step: 733, train_loss: 0.39053112268447876\n",
      "step: 734, train_loss: 0.3429774045944214\n",
      "step: 735, train_loss: 0.18350864946842194\n",
      "step: 736, train_loss: 0.23328934609889984\n",
      "step: 737, train_loss: 0.4150257706642151\n",
      "step: 738, train_loss: 0.1861967295408249\n",
      "step: 739, train_loss: 0.15161675214767456\n",
      "step: 740, train_loss: 0.6211677193641663\n",
      "step: 741, train_loss: 0.23642350733280182\n",
      "step: 742, train_loss: 0.5257543325424194\n",
      "step: 743, train_loss: 0.22259685397148132\n",
      "step: 744, train_loss: 0.2265394926071167\n",
      "step: 745, train_loss: 0.24341881275177002\n",
      "step: 746, train_loss: 0.19365477561950684\n",
      "step: 747, train_loss: 0.25870734453201294\n",
      "step: 748, train_loss: 0.26582610607147217\n",
      "step: 749, train_loss: 0.23961228132247925\n",
      "step: 750, train_loss: 0.21455757319927216\n",
      "step: 751, train_loss: 0.20919658243656158\n",
      "step: 752, train_loss: 0.5375622510910034\n",
      "step: 753, train_loss: 0.5467129349708557\n",
      "step: 754, train_loss: 0.2759535014629364\n",
      "step: 755, train_loss: 0.24081623554229736\n",
      "step: 756, train_loss: 0.23799489438533783\n",
      "step: 757, train_loss: 0.2282334566116333\n",
      "step: 758, train_loss: 0.16398638486862183\n",
      "step: 759, train_loss: 0.26457303762435913\n",
      "step: 760, train_loss: 0.18233497440814972\n",
      "step: 761, train_loss: 0.21314974129199982\n",
      "step: 762, train_loss: 0.3292524218559265\n",
      "step: 763, train_loss: 0.20876644551753998\n",
      "step: 764, train_loss: 0.3956619203090668\n",
      "step: 765, train_loss: 0.5351927280426025\n",
      "step: 766, train_loss: 0.35196611285209656\n",
      "step: 767, train_loss: 0.3138638734817505\n",
      "step: 768, train_loss: 0.17296555638313293\n",
      "step: 769, train_loss: 0.4089301824569702\n",
      "step: 770, train_loss: 0.4665251076221466\n",
      "step: 771, train_loss: 0.2331503927707672\n",
      "step: 772, train_loss: 0.1441006064414978\n",
      "step: 773, train_loss: 0.2569156587123871\n",
      "step: 774, train_loss: 0.28227031230926514\n",
      "step: 775, train_loss: 0.2303084135055542\n",
      "step: 776, train_loss: 0.3686360716819763\n",
      "step: 777, train_loss: 0.21651992201805115\n",
      "step: 778, train_loss: 0.47069603204727173\n",
      "step: 779, train_loss: 0.2885059416294098\n",
      "step: 780, train_loss: 0.619931697845459\n",
      "step: 781, train_loss: 0.22383956611156464\n",
      "step: 782, train_loss: 0.3022785484790802\n",
      "step: 783, train_loss: 0.3725241422653198\n",
      "step: 784, train_loss: 0.19892816245555878\n",
      "step: 785, train_loss: 0.1780877709388733\n",
      "step: 786, train_loss: 0.2480730414390564\n",
      "step: 787, train_loss: 0.22117628157138824\n",
      "step: 788, train_loss: 0.22602207958698273\n",
      "step: 789, train_loss: 0.4342336654663086\n",
      "step: 790, train_loss: 0.2884247899055481\n",
      "step: 791, train_loss: 0.17582237720489502\n",
      "step: 792, train_loss: 0.1710568368434906\n",
      "step: 793, train_loss: 0.5895551443099976\n",
      "step: 794, train_loss: 0.2536036968231201\n",
      "step: 795, train_loss: 0.14432814717292786\n",
      "step: 796, train_loss: 0.11362563073635101\n",
      "step: 797, train_loss: 0.20719560980796814\n",
      "step: 798, train_loss: 0.13562622666358948\n",
      "step: 799, train_loss: 0.3678550720214844\n",
      "step: 800, train_loss: 0.2362392246723175\n",
      "step: 801, train_loss: 0.5410938858985901\n",
      "step: 802, train_loss: 0.13290336728096008\n",
      "step: 803, train_loss: 0.5892388820648193\n",
      "step: 804, train_loss: 0.25085341930389404\n",
      "step: 805, train_loss: 0.19926095008850098\n",
      "step: 806, train_loss: 0.23824898898601532\n",
      "step: 807, train_loss: 0.25259363651275635\n",
      "step: 808, train_loss: 0.33229994773864746\n",
      "step: 809, train_loss: 0.18041710555553436\n",
      "step: 810, train_loss: 0.3506615459918976\n",
      "step: 811, train_loss: 0.24560904502868652\n",
      "step: 812, train_loss: 0.20816218852996826\n",
      "step: 813, train_loss: 0.15802302956581116\n",
      "step: 814, train_loss: 0.10624784231185913\n",
      "step: 815, train_loss: 0.5937625169754028\n",
      "step: 816, train_loss: 0.2974044978618622\n",
      "step: 817, train_loss: 0.43127721548080444\n",
      "step: 818, train_loss: 0.40745455026626587\n",
      "step: 819, train_loss: 0.3621561825275421\n",
      "step: 820, train_loss: 0.3394321799278259\n",
      "step: 821, train_loss: 0.21097742021083832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 822, train_loss: 0.20410215854644775\n",
      "step: 823, train_loss: 0.1889324188232422\n",
      "step: 824, train_loss: 0.2196887880563736\n",
      "step: 825, train_loss: 0.4603906571865082\n",
      "step: 826, train_loss: 0.20757940411567688\n",
      "step: 827, train_loss: 0.3443073332309723\n",
      "step: 828, train_loss: 0.36068934202194214\n",
      "step: 829, train_loss: 0.28228893876075745\n",
      "step: 830, train_loss: 0.3012748658657074\n",
      "step: 831, train_loss: 0.5194291472434998\n",
      "step: 832, train_loss: 0.6296333074569702\n",
      "step: 833, train_loss: 0.1747715175151825\n",
      "step: 834, train_loss: 0.22036001086235046\n",
      "step: 835, train_loss: 0.15081924200057983\n",
      "step: 836, train_loss: 0.5396764874458313\n",
      "step: 837, train_loss: 0.21151328086853027\n",
      "step: 838, train_loss: 0.24379168450832367\n",
      "step: 839, train_loss: 0.27113819122314453\n",
      "step: 840, train_loss: 0.22049781680107117\n",
      "step: 841, train_loss: 0.5321215987205505\n",
      "step: 842, train_loss: 0.3399600386619568\n",
      "step: 843, train_loss: 0.25596368312835693\n",
      "step: 844, train_loss: 0.15425148606300354\n",
      "step: 845, train_loss: 0.1963406354188919\n",
      "step: 846, train_loss: 0.3094249963760376\n",
      "step: 847, train_loss: 0.27716711163520813\n",
      "step: 848, train_loss: 0.5871042013168335\n",
      "step: 849, train_loss: 0.4044756293296814\n",
      "step: 850, train_loss: 0.32195109128952026\n",
      "step: 851, train_loss: 0.1542961597442627\n",
      "step: 852, train_loss: 0.38131844997406006\n",
      "step: 853, train_loss: 0.4667471647262573\n",
      "step: 854, train_loss: 0.19278931617736816\n",
      "step: 855, train_loss: 0.24503667652606964\n",
      "step: 856, train_loss: 0.1626725196838379\n",
      "step: 857, train_loss: 0.16649824380874634\n",
      "step: 858, train_loss: 0.43191421031951904\n",
      "step: 859, train_loss: 0.2037697285413742\n",
      "step: 860, train_loss: 0.20339688658714294\n",
      "step: 861, train_loss: 0.21922487020492554\n",
      "step: 862, train_loss: 0.4924575388431549\n",
      "step: 863, train_loss: 0.36991697549819946\n",
      "step: 864, train_loss: 0.2514588236808777\n",
      "step: 865, train_loss: 0.24133935570716858\n",
      "step: 866, train_loss: 0.3754016160964966\n",
      "step: 867, train_loss: 0.2014526128768921\n",
      "step: 868, train_loss: 0.20986229181289673\n",
      "step: 869, train_loss: 0.2997279167175293\n",
      "step: 870, train_loss: 0.15041327476501465\n",
      "step: 871, train_loss: 0.5727362036705017\n",
      "step: 872, train_loss: 0.21193695068359375\n",
      "step: 873, train_loss: 0.21414998173713684\n",
      "step: 874, train_loss: 0.4505664110183716\n",
      "step: 875, train_loss: 0.3045142590999603\n",
      "step: 876, train_loss: 0.26971322298049927\n",
      "step: 877, train_loss: 0.1678762137889862\n",
      "step: 878, train_loss: 0.2264811247587204\n",
      "step: 879, train_loss: 0.3170740604400635\n",
      "step: 880, train_loss: 0.3248817026615143\n",
      "step: 881, train_loss: 0.22036801278591156\n",
      "step: 882, train_loss: 0.1980208307504654\n",
      "step: 883, train_loss: 0.45092177391052246\n",
      "step: 884, train_loss: 0.46479564905166626\n",
      "step: 885, train_loss: 0.2541959881782532\n",
      "step: 886, train_loss: 0.4431232213973999\n",
      "step: 887, train_loss: 0.19504618644714355\n",
      "step: 888, train_loss: 0.2618289291858673\n",
      "step: 889, train_loss: 0.1886899769306183\n",
      "step: 890, train_loss: 0.3549579977989197\n",
      "step: 891, train_loss: 0.21071311831474304\n",
      "step: 892, train_loss: 0.2854729890823364\n",
      "step: 893, train_loss: 0.2510698437690735\n",
      "step: 894, train_loss: 0.34092336893081665\n",
      "step: 895, train_loss: 0.31174129247665405\n",
      "step: 896, train_loss: 0.28820353746414185\n",
      "step: 897, train_loss: 0.41848576068878174\n",
      "step: 898, train_loss: 0.17620378732681274\n",
      "step: 899, train_loss: 0.2004285454750061\n",
      "step: 900, train_loss: 0.1016266793012619\n",
      "step: 901, train_loss: 0.23496957123279572\n",
      "step: 902, train_loss: 0.18058520555496216\n",
      "step: 903, train_loss: 0.7475578188896179\n",
      "step: 904, train_loss: 0.366848886013031\n",
      "step: 905, train_loss: 0.20222245156764984\n",
      "step: 906, train_loss: 0.3831480145454407\n",
      "step: 907, train_loss: 0.3208617866039276\n",
      "step: 908, train_loss: 0.26993370056152344\n",
      "step: 909, train_loss: 0.13149751722812653\n",
      "step: 910, train_loss: 0.21957287192344666\n",
      "step: 911, train_loss: 0.1427973508834839\n",
      "step: 912, train_loss: 0.5552624464035034\n",
      "step: 913, train_loss: 0.3995122015476227\n",
      "step: 914, train_loss: 0.27079564332962036\n",
      "step: 915, train_loss: 0.5963654518127441\n",
      "step: 916, train_loss: 0.25862956047058105\n",
      "step: 917, train_loss: 0.1368481069803238\n",
      "step: 918, train_loss: 0.2980380058288574\n",
      "step: 919, train_loss: 0.27076053619384766\n",
      "step: 920, train_loss: 0.2538977861404419\n",
      "step: 921, train_loss: 0.13535767793655396\n",
      "step: 922, train_loss: 0.17944064736366272\n",
      "step: 923, train_loss: 0.22855441272258759\n",
      "step: 924, train_loss: 0.23061780631542206\n",
      "step: 925, train_loss: 0.17643900215625763\n",
      "step: 926, train_loss: 0.18243065476417542\n",
      "step: 927, train_loss: 0.37210121750831604\n",
      "step: 928, train_loss: 0.16238418221473694\n",
      "step: 929, train_loss: 0.28257548809051514\n",
      "step: 930, train_loss: 0.31701141595840454\n",
      "step: 931, train_loss: 0.25208377838134766\n",
      "step: 932, train_loss: 0.4658527076244354\n",
      "step: 933, train_loss: 0.49146324396133423\n",
      "step: 934, train_loss: 0.2585943341255188\n",
      "step: 935, train_loss: 0.3398282825946808\n",
      "step: 936, train_loss: 0.3242942988872528\n",
      "step: 937, train_loss: 0.13717597723007202\n",
      "step: 938, train_loss: 0.1768237054347992\n",
      "step: 939, train_loss: 0.42538413405418396\n",
      "step: 940, train_loss: 0.15323375165462494\n",
      "step: 941, train_loss: 0.2725382447242737\n",
      "step: 942, train_loss: 0.4005132019519806\n",
      "step: 943, train_loss: 0.2625212073326111\n",
      "step: 944, train_loss: 0.3602728247642517\n",
      "step: 945, train_loss: 0.24413731694221497\n",
      "step: 946, train_loss: 0.2559892535209656\n",
      "step: 947, train_loss: 0.5913636684417725\n",
      "step: 948, train_loss: 0.44292983412742615\n",
      "step: 949, train_loss: 0.3042801320552826\n",
      "step: 950, train_loss: 0.1941961944103241\n",
      "step: 951, train_loss: 0.3685697913169861\n",
      "step: 952, train_loss: 0.3363152742385864\n",
      "step: 953, train_loss: 0.11336064338684082\n",
      "step: 954, train_loss: 0.22904255986213684\n",
      "step: 955, train_loss: 0.3299373984336853\n",
      "step: 956, train_loss: 0.26579606533050537\n",
      "step: 957, train_loss: 0.2621240019798279\n",
      "step: 958, train_loss: 0.472098708152771\n",
      "step: 959, train_loss: 0.4629322290420532\n",
      "step: 960, train_loss: 0.5255953073501587\n",
      "step: 961, train_loss: 0.35762956738471985\n",
      "step: 962, train_loss: 0.1434810608625412\n",
      "step: 963, train_loss: 0.2834461033344269\n",
      "step: 964, train_loss: 0.6470252275466919\n",
      "step: 965, train_loss: 0.41153183579444885\n",
      "step: 966, train_loss: 0.22974878549575806\n",
      "step: 967, train_loss: 0.4681606888771057\n",
      "step: 968, train_loss: 0.20601032674312592\n",
      "step: 969, train_loss: 0.3899848163127899\n",
      "step: 970, train_loss: 0.13217243552207947\n",
      "step: 971, train_loss: 0.2638712227344513\n",
      "step: 972, train_loss: 0.3408352732658386\n",
      "step: 973, train_loss: 0.20207351446151733\n",
      "step: 974, train_loss: 0.17674706876277924\n",
      "step: 975, train_loss: 0.3470540940761566\n",
      "step: 976, train_loss: 0.4284045696258545\n",
      "step: 977, train_loss: 0.19784128665924072\n",
      "step: 978, train_loss: 0.3015778064727783\n",
      "step: 979, train_loss: 0.3857307434082031\n",
      "step: 980, train_loss: 0.1211903765797615\n",
      "step: 981, train_loss: 0.10218487679958344\n",
      "step: 982, train_loss: 0.1391562521457672\n",
      "step: 983, train_loss: 0.24476131796836853\n",
      "step: 984, train_loss: 0.5618005394935608\n",
      "step: 985, train_loss: 0.26336801052093506\n",
      "step: 986, train_loss: 0.4016868770122528\n",
      "step: 987, train_loss: 0.5375177264213562\n",
      "step: 988, train_loss: 0.4602276086807251\n",
      "step: 989, train_loss: 0.26998794078826904\n",
      "step: 990, train_loss: 0.3080862760543823\n",
      "step: 991, train_loss: 0.24501323699951172\n",
      "step: 992, train_loss: 0.10096491128206253\n",
      "step: 993, train_loss: 0.3651564121246338\n",
      "step: 994, train_loss: 0.503009021282196\n",
      "step: 995, train_loss: 0.21021868288516998\n",
      "step: 996, train_loss: 0.40402233600616455\n",
      "step: 997, train_loss: 0.17623448371887207\n",
      "step: 998, train_loss: 0.14268408715724945\n",
      "step: 999, train_loss: 0.27028781175613403\n",
      "step: 1000, train_loss: 0.1315760612487793\n",
      "step: 1001, train_loss: 0.5096172094345093\n",
      "step: 1002, train_loss: 0.270169198513031\n",
      "step: 1003, train_loss: 0.2077053189277649\n",
      "step: 1004, train_loss: 0.2872812747955322\n",
      "step: 1005, train_loss: 0.19022049009799957\n",
      "step: 1006, train_loss: 0.17308378219604492\n",
      "step: 1007, train_loss: 0.534778892993927\n",
      "step: 1008, train_loss: 0.26933836936950684\n",
      "step: 1009, train_loss: 0.19207462668418884\n",
      "step: 1010, train_loss: 0.45223748683929443\n",
      "step: 1011, train_loss: 0.21199160814285278\n",
      "step: 1012, train_loss: 0.1535513997077942\n",
      "step: 1013, train_loss: 0.2671283781528473\n",
      "step: 1014, train_loss: 0.2498428374528885\n",
      "step: 1015, train_loss: 0.25236913561820984\n",
      "step: 1016, train_loss: 0.1635061502456665\n",
      "step: 1017, train_loss: 0.3814694285392761\n",
      "step: 1018, train_loss: 0.276484876871109\n",
      "step: 1019, train_loss: 0.1189311146736145\n",
      "step: 1020, train_loss: 0.17810150980949402\n",
      "step: 1021, train_loss: 0.5003235340118408\n",
      "step: 1022, train_loss: 0.15444649755954742\n",
      "step: 1023, train_loss: 0.31850945949554443\n",
      "step: 1024, train_loss: 0.5516223311424255\n",
      "step: 1025, train_loss: 0.20956039428710938\n",
      "step: 1026, train_loss: 0.3996981084346771\n",
      "step: 1027, train_loss: 0.2107653170824051\n",
      "step: 1028, train_loss: 0.24655671417713165\n",
      "step: 1029, train_loss: 0.28314414620399475\n",
      "step: 1030, train_loss: 0.17617154121398926\n",
      "step: 1031, train_loss: 0.22563910484313965\n",
      "step: 1032, train_loss: 0.15832026302814484\n",
      "step: 1033, train_loss: 0.29883211851119995\n",
      "step: 1034, train_loss: 0.3473284840583801\n",
      "step: 1035, train_loss: 0.5514163970947266\n",
      "step: 1036, train_loss: 0.1452654004096985\n",
      "step: 1037, train_loss: 0.6191293001174927\n",
      "step: 1038, train_loss: 0.1787262260913849\n",
      "step: 1039, train_loss: 0.4098946154117584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1040, train_loss: 0.33052343130111694\n",
      "step: 1041, train_loss: 0.35218024253845215\n",
      "step: 1042, train_loss: 0.10492303222417831\n",
      "step: 1043, train_loss: 0.19614559412002563\n",
      "step: 1044, train_loss: 0.22317419946193695\n",
      "step: 1045, train_loss: 0.21263869106769562\n",
      "step: 1046, train_loss: 0.5240539908409119\n",
      "step: 1047, train_loss: 0.25581812858581543\n",
      "step: 1048, train_loss: 0.4670151174068451\n",
      "step: 1049, train_loss: 0.3727512061595917\n",
      "step: 1050, train_loss: 0.2629610300064087\n",
      "step: 1051, train_loss: 0.22346968948841095\n",
      "step: 1052, train_loss: 0.1627078801393509\n",
      "step: 1053, train_loss: 0.1544293463230133\n",
      "step: 1054, train_loss: 0.2090354859828949\n",
      "step: 1055, train_loss: 0.24574433267116547\n",
      "step: 1056, train_loss: 0.5191115736961365\n",
      "step: 1057, train_loss: 0.18802635371685028\n",
      "step: 1058, train_loss: 0.4988161027431488\n",
      "step: 1059, train_loss: 0.24234847724437714\n",
      "step: 1060, train_loss: 0.1852511763572693\n",
      "step: 1061, train_loss: 0.21938839554786682\n",
      "step: 1062, train_loss: 0.2282642126083374\n",
      "step: 1063, train_loss: 0.29760438203811646\n",
      "step: 1064, train_loss: 0.22933918237686157\n",
      "step: 1065, train_loss: 0.6118285655975342\n",
      "step: 1066, train_loss: 0.22702929377555847\n",
      "step: 1067, train_loss: 0.15153303742408752\n",
      "step: 1068, train_loss: 0.5035918354988098\n",
      "step: 1069, train_loss: 0.3425608277320862\n",
      "step: 1070, train_loss: 0.28585007786750793\n",
      "step: 1071, train_loss: 0.2907068431377411\n",
      "step: 1072, train_loss: 0.1951192170381546\n",
      "step: 1073, train_loss: 0.27612942457199097\n",
      "step: 1074, train_loss: 0.3235265612602234\n",
      "step: 1075, train_loss: 0.29403069615364075\n",
      "step: 1076, train_loss: 0.1697358787059784\n",
      "step: 1077, train_loss: 0.3197548985481262\n",
      "step: 1078, train_loss: 0.6644764542579651\n",
      "step: 1079, train_loss: 0.33011531829833984\n",
      "step: 1080, train_loss: 0.22033342719078064\n",
      "step: 1081, train_loss: 0.2921905517578125\n",
      "step: 1082, train_loss: 0.17484185099601746\n",
      "step: 1083, train_loss: 0.4414719343185425\n",
      "step: 1084, train_loss: 0.17135635018348694\n",
      "step: 1085, train_loss: 0.2500917315483093\n",
      "step: 1086, train_loss: 0.6409441232681274\n",
      "step: 1087, train_loss: 0.3446236848831177\n",
      "step: 1088, train_loss: 0.23926083743572235\n",
      "step: 1089, train_loss: 0.25587522983551025\n",
      "step: 1090, train_loss: 0.15517595410346985\n",
      "step: 1091, train_loss: 0.4039005637168884\n",
      "step: 1092, train_loss: 0.28312551975250244\n",
      "step: 1093, train_loss: 0.26852861046791077\n",
      "step: 1094, train_loss: 0.3256438970565796\n",
      "step: 1095, train_loss: 0.20855244994163513\n",
      "step: 1096, train_loss: 0.20097246766090393\n",
      "step: 1097, train_loss: 0.47648173570632935\n",
      "step: 1098, train_loss: 0.32591983675956726\n",
      "step: 1099, train_loss: 0.24079984426498413\n",
      "step: 1100, train_loss: 0.17367370426654816\n",
      "step: 1101, train_loss: 0.1168200820684433\n",
      "step: 1102, train_loss: 0.4156542420387268\n",
      "step: 1103, train_loss: 0.38012832403182983\n",
      "step: 1104, train_loss: 0.2570847272872925\n",
      "step: 1105, train_loss: 0.3914521038532257\n",
      "step: 1106, train_loss: 0.24298842251300812\n",
      "step: 1107, train_loss: 0.2350144237279892\n",
      "step: 1108, train_loss: 0.15255063772201538\n",
      "step: 1109, train_loss: 0.3551146388053894\n",
      "step: 1110, train_loss: 0.5260041356086731\n",
      "step: 1111, train_loss: 0.4580439031124115\n",
      "step: 1112, train_loss: 0.22355684638023376\n",
      "step: 1113, train_loss: 0.13107936084270477\n",
      "step: 1114, train_loss: 0.2697101831436157\n",
      "step: 1115, train_loss: 0.28774046897888184\n",
      "step: 1116, train_loss: 0.13433656096458435\n",
      "step: 1117, train_loss: 0.20420560240745544\n",
      "step: 1118, train_loss: 0.16660934686660767\n",
      "step: 1119, train_loss: 0.3485300540924072\n",
      "step: 1120, train_loss: 0.16369426250457764\n",
      "step: 1121, train_loss: 0.27240610122680664\n",
      "step: 1122, train_loss: 0.2326284945011139\n",
      "step: 1123, train_loss: 0.3942078948020935\n",
      "step: 1124, train_loss: 0.4281732439994812\n",
      "step: 1125, train_loss: 0.3533945679664612\n",
      "step: 1126, train_loss: 0.26098906993865967\n",
      "step: 1127, train_loss: 0.4920331835746765\n",
      "step: 1128, train_loss: 0.24184155464172363\n",
      "step: 1129, train_loss: 0.18395200371742249\n",
      "step: 1130, train_loss: 0.2442534863948822\n",
      "step: 1131, train_loss: 0.4397076964378357\n",
      "step: 1132, train_loss: 0.15153853595256805\n",
      "step: 1133, train_loss: 0.28140145540237427\n",
      "step: 1134, train_loss: 0.2735039293766022\n",
      "step: 1135, train_loss: 0.48790204524993896\n",
      "step: 1136, train_loss: 0.3076210618019104\n",
      "step: 1137, train_loss: 0.18006938695907593\n",
      "step: 1138, train_loss: 0.28711292147636414\n",
      "step: 1139, train_loss: 0.14261484146118164\n",
      "step: 1140, train_loss: 0.2016095519065857\n",
      "step: 1141, train_loss: 0.5646575689315796\n",
      "step: 1142, train_loss: 0.47894957661628723\n",
      "step: 1143, train_loss: 0.1821083277463913\n",
      "step: 1144, train_loss: 0.16529324650764465\n",
      "step: 1145, train_loss: 0.1890416294336319\n",
      "step: 1146, train_loss: 0.3945169448852539\n",
      "step: 1147, train_loss: 0.603699803352356\n",
      "step: 1148, train_loss: 0.2970770597457886\n",
      "step: 1149, train_loss: 0.3618103861808777\n",
      "step: 1150, train_loss: 0.19303014874458313\n",
      "step: 1151, train_loss: 0.16818518936634064\n",
      "step: 1152, train_loss: 0.24953874945640564\n",
      "step: 1153, train_loss: 0.5242023468017578\n",
      "step: 1154, train_loss: 0.3231804370880127\n",
      "step: 1155, train_loss: 0.34271425008773804\n",
      "step: 1156, train_loss: 0.15346544981002808\n",
      "step: 1157, train_loss: 0.12605977058410645\n",
      "step: 1158, train_loss: 0.09769750386476517\n",
      "step: 1159, train_loss: 0.279806911945343\n",
      "step: 1160, train_loss: 0.37660887837409973\n",
      "step: 1161, train_loss: 0.26991456747055054\n",
      "step: 1162, train_loss: 0.600237250328064\n",
      "step: 1163, train_loss: 0.3234703242778778\n",
      "step: 1164, train_loss: 0.4998166561126709\n",
      "step: 1165, train_loss: 0.2244163155555725\n",
      "step: 1166, train_loss: 0.2039932906627655\n",
      "step: 1167, train_loss: 0.3474380373954773\n",
      "step: 1168, train_loss: 0.232509583234787\n",
      "step: 1169, train_loss: 0.3335030972957611\n",
      "step: 1170, train_loss: 0.588519811630249\n",
      "step: 1171, train_loss: 0.3650902211666107\n",
      "step: 1172, train_loss: 0.16106417775154114\n",
      "step: 1173, train_loss: 0.27086901664733887\n",
      "step: 1174, train_loss: 0.23789355158805847\n",
      "step: 1175, train_loss: 0.19393952190876007\n",
      "step: 1176, train_loss: 0.1484019160270691\n",
      "step: 1177, train_loss: 0.42170944809913635\n",
      "step: 1178, train_loss: 0.27005916833877563\n",
      "step: 1179, train_loss: 0.16303306818008423\n",
      "step: 1180, train_loss: 0.19402101635932922\n",
      "step: 1181, train_loss: 0.39447465538978577\n",
      "step: 1182, train_loss: 0.16398198902606964\n",
      "step: 1183, train_loss: 0.16958948969841003\n",
      "step: 1184, train_loss: 0.26083996891975403\n",
      "step: 1185, train_loss: 0.4366939663887024\n",
      "step: 1186, train_loss: 0.46158015727996826\n",
      "step: 1187, train_loss: 0.25084418058395386\n",
      "step: 1188, train_loss: 0.24923858046531677\n",
      "step: 1189, train_loss: 0.33773013949394226\n",
      "step: 1190, train_loss: 0.21016541123390198\n",
      "step: 1191, train_loss: 0.12945038080215454\n",
      "step: 1192, train_loss: 0.1334744244813919\n",
      "step: 1193, train_loss: 0.347992479801178\n",
      "step: 1194, train_loss: 0.1767752468585968\n",
      "step: 1195, train_loss: 0.5415130853652954\n",
      "step: 1196, train_loss: 0.5139236450195312\n",
      "step: 1197, train_loss: 0.25190436840057373\n",
      "step: 1198, train_loss: 0.2149699330329895\n",
      "step: 1199, train_loss: 0.21421663463115692\n",
      "step: 1200, train_loss: 0.316079705953598\n",
      "step: 1201, train_loss: 0.15403106808662415\n",
      "step: 1202, train_loss: 0.16838809847831726\n",
      "step: 1203, train_loss: 0.5710436105728149\n",
      "step: 1204, train_loss: 0.49656087160110474\n",
      "step: 1205, train_loss: 0.11668075621128082\n",
      "step: 1206, train_loss: 0.2976105213165283\n",
      "step: 1207, train_loss: 0.22627171874046326\n",
      "step: 1208, train_loss: 0.16666775941848755\n",
      "step: 1209, train_loss: 0.15019437670707703\n",
      "step: 1210, train_loss: 0.4553252160549164\n",
      "step: 1211, train_loss: 0.2695932984352112\n",
      "step: 1212, train_loss: 0.18929821252822876\n",
      "step: 1213, train_loss: 0.34227001667022705\n",
      "step: 1214, train_loss: 0.16109803318977356\n",
      "step: 1215, train_loss: 0.46375247836112976\n",
      "step: 1216, train_loss: 0.20699390769004822\n",
      "step: 1217, train_loss: 0.27198565006256104\n",
      "step: 1218, train_loss: 0.15157195925712585\n",
      "step: 1219, train_loss: 0.27233511209487915\n",
      "step: 1220, train_loss: 0.38623037934303284\n",
      "step: 1221, train_loss: 0.21414439380168915\n",
      "step: 1222, train_loss: 0.20845603942871094\n",
      "step: 1223, train_loss: 0.1902497559785843\n",
      "step: 1224, train_loss: 0.2877868413925171\n",
      "step: 1225, train_loss: 0.6037441492080688\n",
      "step: 1226, train_loss: 0.4109562039375305\n",
      "step: 1227, train_loss: 0.17115366458892822\n",
      "step: 1228, train_loss: 0.29304376244544983\n",
      "step: 1229, train_loss: 0.21916572749614716\n",
      "step: 1230, train_loss: 0.5147980451583862\n",
      "step: 1231, train_loss: 0.2897067964076996\n",
      "step: 1232, train_loss: 0.3099268078804016\n",
      "step: 1233, train_loss: 0.2649848163127899\n",
      "step: 1234, train_loss: 0.18298542499542236\n",
      "step: 1235, train_loss: 0.3082495331764221\n",
      "step: 1236, train_loss: 0.4020690321922302\n",
      "step: 1237, train_loss: 0.4222913980484009\n",
      "step: 1238, train_loss: 0.2560829520225525\n",
      "step: 1239, train_loss: 0.17105035483837128\n",
      "step: 1240, train_loss: 0.32911884784698486\n",
      "step: 1241, train_loss: 0.31251272559165955\n",
      "step: 1242, train_loss: 0.4057072401046753\n",
      "step: 1243, train_loss: 0.5623691082000732\n",
      "step: 1244, train_loss: 0.3697240948677063\n",
      "step: 1245, train_loss: 0.21001291275024414\n",
      "step: 1246, train_loss: 0.16078492999076843\n",
      "step: 1247, train_loss: 0.5368460416793823\n",
      "step: 1248, train_loss: 0.15740589797496796\n",
      "step: 1249, train_loss: 0.27887454628944397\n",
      "step: 1250, train_loss: 0.33395612239837646\n",
      "step: 1251, train_loss: 0.20364214479923248\n",
      "step: 1252, train_loss: 0.36846041679382324\n",
      "step: 1253, train_loss: 0.5514737367630005\n",
      "step: 1254, train_loss: 0.24789980053901672\n",
      "step: 1255, train_loss: 0.17380449175834656\n",
      "step: 1256, train_loss: 0.26212233304977417\n",
      "step: 1257, train_loss: 0.4739668667316437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1258, train_loss: 0.2492714375257492\n",
      "step: 1259, train_loss: 0.3447043001651764\n",
      "step: 1260, train_loss: 0.11174464225769043\n",
      "step: 1261, train_loss: 0.1824822723865509\n",
      "step: 1262, train_loss: 0.26380839943885803\n",
      "step: 1263, train_loss: 0.3814055919647217\n",
      "step: 1264, train_loss: 0.198882594704628\n",
      "step: 1265, train_loss: 0.5457421541213989\n",
      "step: 1266, train_loss: 0.24775193631649017\n",
      "step: 1267, train_loss: 0.2575637698173523\n",
      "step: 1268, train_loss: 0.5060690641403198\n",
      "step: 1269, train_loss: 0.12608584761619568\n",
      "step: 1270, train_loss: 0.262966513633728\n",
      "step: 1271, train_loss: 0.21340428292751312\n",
      "step: 1272, train_loss: 0.2053951919078827\n",
      "step: 1273, train_loss: 0.3181235194206238\n",
      "step: 1274, train_loss: 0.15516111254692078\n",
      "step: 1275, train_loss: 0.1997602880001068\n",
      "step: 1276, train_loss: 0.258098840713501\n",
      "step: 1277, train_loss: 0.22002345323562622\n",
      "step: 1278, train_loss: 0.10333068668842316\n",
      "step: 1279, train_loss: 0.22742587327957153\n",
      "step: 1280, train_loss: 0.13219033181667328\n",
      "step: 1281, train_loss: 0.2579007148742676\n",
      "step: 1282, train_loss: 0.3197774589061737\n",
      "step: 1283, train_loss: 0.41039201617240906\n",
      "step: 1284, train_loss: 0.4776812493801117\n",
      "step: 1285, train_loss: 0.4227800965309143\n",
      "step: 1286, train_loss: 0.6155037879943848\n",
      "step: 1287, train_loss: 0.45182856917381287\n",
      "step: 1288, train_loss: 0.2570510804653168\n",
      "step: 1289, train_loss: 0.17592036724090576\n",
      "step: 1290, train_loss: 0.4009735882282257\n",
      "step: 1291, train_loss: 0.3094272017478943\n",
      "step: 1292, train_loss: 0.32993727922439575\n",
      "step: 1293, train_loss: 0.49398571252822876\n",
      "step: 1294, train_loss: 0.3955574035644531\n",
      "step: 1295, train_loss: 0.19447526335716248\n",
      "step: 1296, train_loss: 0.20562675595283508\n",
      "step: 1297, train_loss: 0.1868555247783661\n",
      "step: 1298, train_loss: 0.29134708642959595\n",
      "step: 1299, train_loss: 0.2894331216812134\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_batches(x, y, batch_size):\n",
    "    n_data = len(x)\n",
    "    indices = np.arange(n_data)\n",
    "    np.random.shuffle(indices)\n",
    "    x_shuffled = x[indices]\n",
    "    y_shuffled = y[indices]\n",
    "    \n",
    "    # 元データからランダムに batch_size 個ずつ抽出する\n",
    "    for i in range(0, n_data, batch_size):\n",
    "        x_batch = x_shuffled[i: i + batch_size]\n",
    "        y_batch = y_shuffled[i: i + batch_size]\n",
    "        yield x_batch, y_batch\n",
    "\n",
    "def main():\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data()\n",
    "    \n",
    "    # 3.6.5 データの前処理\n",
    "    \n",
    "    ## 平均と標準偏差を計算する\n",
    "    \n",
    "    x_train_mean = x_train.mean(axis=0)\n",
    "    x_train_std = x_train.std(axis=0)\n",
    "    \n",
    "    y_train_mean = y_train.mean(axis=0)\n",
    "    y_train_std = y_train.std(axis=0)\n",
    "    \n",
    "    ## 標準化\n",
    "\n",
    "    x_train = (x_train - x_train_mean) / x_train_std\n",
    "    y_train = (y_train - y_train_mean) / y_train_std\n",
    "\n",
    "    ## x_test に対しても x_train_mean と x_train_std を使う\n",
    "    \n",
    "    x_test = (x_test - x_train_mean) / x_train_std\n",
    "    \n",
    "    ## y_test に対しても y_train_mean と y_train_std を使う\n",
    "\n",
    "    y_test = (y_test - y_train_mean) / y_train_std\n",
    "    \n",
    "    \n",
    "    # 3.6.6 モデルの定義\n",
    "    \n",
    "    ##　説明変数用のプレースホルダー\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, (None, 13), name='x')\n",
    "    y = tf.placeholder(tf.float32, (None, 1), name='y')\n",
    "    \n",
    "    ## 説明変数を重み w で足し合わせただけの簡単なモデル\n",
    "    \n",
    "    w = tf.Variable(tf.random_normal((13,1)))\n",
    "    pred = tf.matmul(x, w)\n",
    "# 3.6.7 損失関数の定義と学習\n",
    "    \n",
    "    ## 実測値と推定値の差の二乗の平均を誤差とする\n",
    "    \n",
    "    loss = tf.reduce_mean((y - pred) ** 2)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(\n",
    "        learning_rate=0.1\n",
    "    )\n",
    "    train_step = optimizer.minimize(loss)\n",
    "    \n",
    "    # ミニバッチのサイズ\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    step = 0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # 100エポック回す\n",
    "        for epoch in range(100):\n",
    "            for x_batch, y_batch in get_batches(x_train, y_train, 32):\n",
    "                train_loss, _ = sess.run(\n",
    "                    [loss, train_step],\n",
    "                    feed_dict={\n",
    "                        x: x_batch,\n",
    "                        y: y_batch.reshape((-1, 1))\n",
    "                    }\n",
    "                )\n",
    "                print('step: {}, train_loss: {}'.format(\n",
    "                    step, train_loss\n",
    "                ))\n",
    "                step += 1\n",
    "\n",
    "        pred_ = sess.run(\n",
    "            pred,\n",
    "            feed_dict={\n",
    "                x: x_test\n",
    "            }\n",
    "        )\n",
    "        \n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
