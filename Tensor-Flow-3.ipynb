{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD+mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (60000, 28, 28)\n",
      "y_train.shape: (60000,)\n",
      "x_test.shape: (10000, 28, 28)\n",
      "y_test.shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# インポートしたデータの形を確認\n",
    "print('x_train.shape:', x_train.shape)\n",
    "print('y_train.shape:', y_train.shape)\n",
    "print('x_test.shape:', x_test.shape)\n",
    "print('y_test.shape:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000, 784)\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_test = x_test / 255.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (60000, 784)\n",
      "y_train.shape: (60000,)\n",
      "x_test.shape: (10000, 784)\n",
      "y_test.shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print('x_train.shape:', x_train.shape)\n",
    "print('y_train.shape:', y_train.shape)\n",
    "print('x_test.shape:', x_test.shape)\n",
    "print('y_test.shape:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "model = Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Dense\n",
    "model.add(\n",
    "\tDense(\n",
    "\t\tunits=64,\n",
    "\t\tinput_shape=(784,),\n",
    "\t\tactivation='relu'\n",
    "\t)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(\n",
    "\tDense(\n",
    "\t\tunits=10,\n",
    "\t\tactivation='softmax'\n",
    "\t)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-12-23 17:19:10--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
      "Resolving bin.equinox.io (bin.equinox.io)... 52.20.12.96, 52.3.53.115, 34.206.134.194, ...\n",
      "Connecting to bin.equinox.io (bin.equinox.io)|52.20.12.96|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13773305 (13M) [application/octet-stream]\n",
      "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
      "\n",
      "ngrok-stable-linux- 100%[===================>]  13.13M  1.24MB/s    in 16s     \n",
      "\n",
      "2019-12-23 17:19:28 (820 KB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [13773305/13773305]\n",
      "\n",
      "Archive:  ngrok-stable-linux-amd64.zip\n",
      "  inflating: ngrok                   \n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "!wget -F https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "!unzip -o ngrok-stable-linux-amd64.zip\n",
    "LOG_DIR = './log'\n",
    "get_ipython().system_raw(\n",
    "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
    "    .format(LOG_DIR)\n",
    ")\n",
    "get_ipython().system_raw('./ngrok http 6006 &')\n",
    "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n",
      "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 7s 152us/sample - loss: 0.3355 - accuracy: 0.9065 - val_loss: 0.1897 - val_accuracy: 0.9479\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 5s 102us/sample - loss: 0.1650 - accuracy: 0.9516 - val_loss: 0.1442 - val_accuracy: 0.9592\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 5s 106us/sample - loss: 0.1227 - accuracy: 0.9642 - val_loss: 0.1209 - val_accuracy: 0.9650\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 5s 100us/sample - loss: 0.0984 - accuracy: 0.9716 - val_loss: 0.1122 - val_accuracy: 0.9679\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 5s 107us/sample - loss: 0.0805 - accuracy: 0.9760 - val_loss: 0.1089 - val_accuracy: 0.9680\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 5s 100us/sample - loss: 0.0666 - accuracy: 0.9801 - val_loss: 0.0997 - val_accuracy: 0.9709\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 5s 106us/sample - loss: 0.0554 - accuracy: 0.9833 - val_loss: 0.0977 - val_accuracy: 0.9716\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 6s 118us/sample - loss: 0.0475 - accuracy: 0.9854 - val_loss: 0.0994 - val_accuracy: 0.9694\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 6s 117us/sample - loss: 0.0399 - accuracy: 0.9881 - val_loss: 0.0951 - val_accuracy: 0.9722\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 6s 126us/sample - loss: 0.0345 - accuracy: 0.9898 - val_loss: 0.0993 - val_accuracy: 0.9730\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 5s 108us/sample - loss: 0.0309 - accuracy: 0.9907 - val_loss: 0.0969 - val_accuracy: 0.9729\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 5s 108us/sample - loss: 0.0262 - accuracy: 0.9918 - val_loss: 0.1039 - val_accuracy: 0.9723\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 5s 104us/sample - loss: 0.0224 - accuracy: 0.9939 - val_loss: 0.1019 - val_accuracy: 0.9718\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 6s 123us/sample - loss: 0.0198 - accuracy: 0.9948 - val_loss: 0.1060 - val_accuracy: 0.9742\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 5s 100us/sample - loss: 0.0174 - accuracy: 0.9946 - val_loss: 0.1067 - val_accuracy: 0.9734\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 5s 98us/sample - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.1076 - val_accuracy: 0.9736\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0124 - accuracy: 0.9965 - val_loss: 0.1071 - val_accuracy: 0.9737\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0127 - accuracy: 0.9966 - val_loss: 0.1176 - val_accuracy: 0.9722\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 5s 98us/sample - loss: 0.0117 - accuracy: 0.9968 - val_loss: 0.1270 - val_accuracy: 0.9712\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 5s 107us/sample - loss: 0.0093 - accuracy: 0.9973 - val_loss: 0.1182 - val_accuracy: 0.9730\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.datasets import mnist\n",
    "from tensorflow.python.keras.utils import to_categorical\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# スケール変換\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_test = x_test / 255.\n",
    "\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "\tDense(\n",
    "\t\tunits=64,\n",
    "\t\tinput_shape=(784,),\n",
    "\t\tactivation='relu'\n",
    "\t)\n",
    ")\n",
    "model.add(\n",
    "\tDense(\n",
    "\t\tunits=10,\n",
    "\t\tactivation='softmax'\n",
    "\t)\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "\toptimizer='adam',\n",
    "\tloss='categorical_crossentropy',\n",
    "\tmetrics=['accuracy']\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "tsb=TensorBoard(log_dir=LOG_DIR,\n",
    "    histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    write_grads=True,\n",
    "    batch_size=batch_size,\n",
    "    write_images=True)\n",
    "\n",
    "history_adam=model.fit(\n",
    "\tx_train,\n",
    "\ty_train,\n",
    "\tbatch_size=batch_size,\n",
    "\tepochs=20,\n",
    "\tvalidation_split=0.2,\n",
    "\tcallbacks=[tsb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quanhm/.local/lib/python3.6/site-packages/keras/callbacks/tensorboard_v2.py:92: UserWarning: The TensorBoard callback `batch_size` argument (for histogram computation) is deprecated with TensorFlow 2.0. It will be ignored.\n",
      "  warnings.warn('The TensorBoard callback `batch_size` argument '\n",
      "/home/quanhm/.local/lib/python3.6/site-packages/keras/callbacks/tensorboard_v2.py:97: UserWarning: The TensorBoard callback does not support gradients display when using TensorFlow 2.0. The `write_grads` argument is ignored.\n",
      "  warnings.warn('The TensorBoard callback does not support '\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_input to have shape (784,) but got array with shape (13,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2b30e0a1357a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m           callbacks=[tbCallBack])\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         steps=steps)\n\u001b[0m\u001b[1;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[1;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2470\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2471\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2472\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    572\u001b[0m                              \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m                              str(data_shape))\n\u001b[0m\u001b[1;32m    575\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_input to have shape (784,) but got array with shape (13,)"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "tbCallBack = TensorBoard(log_dir='./log', histogram_freq=1,\n",
    "                         write_graph=True,\n",
    "                         write_grads=True,\n",
    "                         batch_size=batch_size,\n",
    "                         write_images=True)\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test),\n",
    "          callbacks=[tbCallBack])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
